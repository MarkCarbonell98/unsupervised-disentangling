{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervised-disentangling-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkCarbonell98/unsupervised-disentangling/blob/demo-branch/unsupervised_disentangling_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWag5ad0GYz1",
        "colab_type": "text"
      },
      "source": [
        "# README\n",
        "\n",
        "In order to demonstrate the use of unsupervised disentangling, follow these steps:\n",
        "\n",
        "1. Run the two cells below this one to setup the repo. It automatically contains three demonstration images. You can upload your demo images in `unsupervised-disentangling/original_code/datasets/deepfashion/images/demo/` using Colab's UI\n",
        "\n",
        "2. You can upload your own demo images in `unsupervised-disentangling/original_code/datasets/deepfashion/images/demo/` using Colab's UI. Please hold the naming convention \"test-model-1.jpg\", \"test-model-2.png\"...\n",
        "\n",
        "3. Run the last two cells to see the comparison between the demo images and their corresponding predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTiYOfSjKZhS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0e1e809f-3da6-44d0-d9ea-4960efd74ead"
      },
      "source": [
        "%%bash\n",
        "rm -rf unsupervised-disentangling/\n",
        "git clone https://github.com/MarkCarbonell98/unsupervised-disentangling.git unsupervised-disentangling \n",
        "cd \"unsupervised-disentangling/\"\n",
        "git checkout demo-branch\n",
        "cd \"original_code/\"\n",
        "pip -q install dotmap tensorflow==1.14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'demo-branch' set up to track remote branch 'demo-branch' from 'origin'.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'unsupervised-disentangling'...\n",
            "Switched to a new branch 'demo-branch'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWykw9xSwBs8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "589f4d6a-d257-4491-dec7-4c40f9b1c373"
      },
      "source": [
        "%%bash\n",
        "cd \"unsupervised-disentangling/original_code\"\n",
        "rm -rf baseline_deepfashion_256*\n",
        "wget --quiet https://heibox.uni-heidelberg.de/f/c2e7b6a77f2f4736a01f/?dl=1 -O baseline_deepfashion_256_checkpoints.tgz\n",
        "tar -xvf baseline_deepfashion_256_checkpoints.tgz\n",
        "mkdir -p -v experiments/baseline_deepfashion_256/demo-predictions\n",
        "cp -r -v -t experiments/baseline_deepfashion_256 release/*\n",
        "rm -rf release/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "release/\n",
            "release/config.txt\n",
            "release/saved_model/\n",
            "release/saved_model/save_net.ckpt-480000.data-00000-of-00001\n",
            "release/saved_model/checkpoint\n",
            "release/saved_model/save_net.ckpt-480000.meta\n",
            "release/saved_model/save_net.ckpt-480000.index\n",
            "mkdir: created directory 'experiments'\n",
            "mkdir: created directory 'experiments/baseline_deepfashion_256'\n",
            "mkdir: created directory 'experiments/baseline_deepfashion_256/demo-predictions'\n",
            "'release/config.txt' -> 'experiments/baseline_deepfashion_256/config.txt'\n",
            "'release/saved_model' -> 'experiments/baseline_deepfashion_256/saved_model'\n",
            "'release/saved_model/save_net.ckpt-480000.data-00000-of-00001' -> 'experiments/baseline_deepfashion_256/saved_model/save_net.ckpt-480000.data-00000-of-00001'\n",
            "'release/saved_model/checkpoint' -> 'experiments/baseline_deepfashion_256/saved_model/checkpoint'\n",
            "'release/saved_model/save_net.ckpt-480000.meta' -> 'experiments/baseline_deepfashion_256/saved_model/save_net.ckpt-480000.meta'\n",
            "'release/saved_model/save_net.ckpt-480000.index' -> 'experiments/baseline_deepfashion_256/saved_model/save_net.ckpt-480000.index'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mbMEIimx0ex",
        "colab_type": "text"
      },
      "source": [
        "# Image Upload and Demo Prediction\n",
        "\n",
        "- Please upload the images you want to predict to the `unsupervised-disentangling/original_code/datasets/deepfashion/images/demo/ ` directory using Colab's UI\n",
        "\n",
        "- There are three test images already saved there. You can view their predictions by running the cell below. \n",
        "\n",
        "- For the comparison plot to be shown side by side, please name the images you upload as \"test-model-1.jpg\", \"test-model-2.jpg\" and so on.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2lPpinAM1Mz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "166d6ef9-a3a1-4420-e4b1-20a43895230b"
      },
      "source": [
        "!cd \"unsupervised-disentangling/original_code/\"; python predict-demo.py baseline_deepfashion_256 \\\n",
        "  --dataset deepfashion --bn 2 --static \\\n",
        "  --in_dim 256 \\\n",
        "  --reconstr_dim 256 \\\n",
        "  --covariance \\\n",
        "  --scal 1.0 \\\n",
        "  --contrast_var 0.01 \\\n",
        "  --brightness_var 0.01 \\\n",
        "  --saturation_var 0.01 \\\n",
        "  --hue_var 0.01 \\\n",
        "  --adversarial \\\n",
        "  --mode infer \\\n",
        "  --pad_size 50 \\\n",
        "  --pck_tolerance 6 2>&1 | grep -vi 'warning'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "  import pandas.util.testing as tm\n",
            "\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2020-09-10 16:00:30.007253: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-09-10 16:00:30.012354: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-09-10 16:00:30.012638: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1752a00 executing computations on platform Host. Devices:\n",
            "2020-09-10 16:00:30.012668: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "(2, 256, 256, 3)\n",
            "Traceback (most recent call last):\n",
            "  File \"predict-demo.py\", line 224, in <module>\n",
            "    main(arg)\n",
            "  File \"predict-demo.py\", line 82, in main\n",
            "    model = Model(orig_images, arg, tps_param_dic)\n",
            "  File \"/content/unsupervised-disentangling/original_code/model.py\", line 153, in __init__\n",
            "    self.optimize\n",
            "  File \"/content/unsupervised-disentangling/original_code/utils.py\", line 62, in decorator\n",
            "    setattr(self, attribute, function(self, *args, **kwargs))\n",
            "  File \"/content/unsupervised-disentangling/original_code/model.py\", line 430, in optimize\n",
            "    transform_loss, precision_loss, l2_loss, d_loss, g_loss = self.loss\n",
            "  File \"/content/unsupervised-disentangling/original_code/utils.py\", line 62, in decorator\n",
            "    setattr(self, attribute, function(self, *args, **kwargs))\n",
            "  File \"/content/unsupervised-disentangling/original_code/model.py\", line 400, in loss\n",
            "    D, D_ = self.t_D[:flatten_dim], self.t_D[flatten_dim:]\n",
            "TypeError: 'NoneType' object is not subscriptable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkuJvR0RFpGp",
        "colab_type": "text"
      },
      "source": [
        "# Image Comparison\n",
        "\n",
        "The comparison results between the original and predicted images is shown below.\n",
        "\n",
        "Just run the two cells underneath this one and see the magic! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZzB7HZ8aGSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from IPython.display import Image\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('classic')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTxEg0vd5EbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " tyorig_img_path = \"/content/unsupervised-disentangling/original_code/datasets/deepfashion/images/demo\"\n",
        "pred_img_path = \"/content/unsupervised-disentangling/original_code/experiments/baseline_deepfashion_256/demo-predictions\"\n",
        "orig_img_list = [mpl.image.imread(orig_img_path + \"/\" + img_name) for img_name in os.listdir(orig_img_path)[::-1]]\n",
        "pred_img_list = [mpl.image.imread(pred_img_path + \"/\" + img_name) for img_name in sorted(os.listdir(pred_img_path))[::-1]]\n",
        "\n",
        "rows = len(orig_img_list)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.title(\"Comparison between original images and keypoint predicted images\")\n",
        "for i in range(rows):\n",
        "  plt.subplot(rows, 2, 2*i+1)\n",
        "  plt.imshow(orig_img_list[i])\n",
        "  plt.subplot(rows, 2, 2*i+2)\n",
        "  plt.imshow(pred_img_list[i])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KfTWSHYVNPO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "da162f5c-51fb-452b-dd39-7e2efe1075eb"
      },
      "source": [
        "%%bash\n",
        "cd /content/unsupervised-disentangling/\n",
        "git status\n",
        "git add original_code/predict-demo.py original_code/utils.py\n",
        "git config user.email \"sandro.braun@iwr.uni-heidelberg.de\"\n",
        "git config user.name \"Sandro Braun\"\n",
        "git commit -m \"fix scaling\"\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch demo-branch\n",
            "Your branch is up to date with 'origin/demo-branch'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\tmodified:   original_code/predict-demo.py\n",
            "\tmodified:   original_code/utils.py\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\tmodified:   original_code/datasets/deepfashion/data_demo.csv\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\toriginal_code/baseline_deepfashion_256_checkpoints.tgz\n",
            "\n",
            "[demo-branch 64d17dc] fix scaling\n",
            " 2 files changed, 158 insertions(+), 18 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA8QdllXpzBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/unsupervised-disentangling/original_code/predict-demo.py /content/unsupervised-disentangling/original_code/transfer-demo.py "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUDoRb4x1XMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47ef9d91-1eba-4e82-c819-a3f81bba9eea"
      },
      "source": [
        "!cd \"unsupervised-disentangling/original_code/\"; python transfer-demo.py baseline_deepfashion_256 \\\n",
        "  --dataset deepfashion --bn 3 --static \\\n",
        "  --in_dim 256 \\\n",
        "  --reconstr_dim 256 \\\n",
        "  --covariance \\\n",
        "  --scal 1.0 \\\n",
        "  --contrast_var 0.01 \\\n",
        "  --brightness_var 0.01 \\\n",
        "  --saturation_var 0.01 \\\n",
        "  --hue_var 0.01 \\\n",
        "  --adversarial \\\n",
        "  --mode infer \\\n",
        "  --pad_size 50 \\\n",
        "  --pck_tolerance 6 2>&1 | grep -vi 'warning'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "  import pandas.util.testing as tm\n",
            "\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2020-09-10 17:02:12.400048: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-09-10 17:02:12.404655: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-09-10 17:02:12.404910: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ef2a00 executing computations on platform Host. Devices:\n",
            "2020-09-10 17:02:12.404939: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "\n",
            "\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "found checkpoint :save_net.ckpt-480000\n",
            "counter set to 480000\n",
            "[]\n",
            "experiments/baseline_deepfashion_256/image/480001_0.png\n",
            "experiments/baseline_deepfashion_256/image/480001_1.png\n",
            "experiments/baseline_deepfashion_256/image/480001_2.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_0.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_1.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_2.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_3.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_4.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_5.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_6.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_7.png\n",
            "experiments/baseline_deepfashion_256/image_rec/480001_8.png\n",
            "End of Prediction\n",
            "Saving outputs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deeomEe91zed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}